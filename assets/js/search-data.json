{
  
    
        "post0": {
            "title": "Chaotic behavior in the damped, harmonically driven pendulum",
            "content": "",
            "url": "https://celis.github.io/personal/jupyter/2020/11/15/chaotic-pendulum.html",
            "relUrl": "/jupyter/2020/11/15/chaotic-pendulum.html",
            "date": " • Nov 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "On neural ordinary differential equations",
            "content": "Introduction . in progress .",
            "url": "https://celis.github.io/personal/jupyter/2020/10/24/on-neural-ode.html",
            "relUrl": "/jupyter/2020/10/24/on-neural-ode.html",
            "date": " • Oct 24, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Modelling Customer Relationships as Markov Chains",
            "content": "Introduction . On this note I want to explore the work presented in the article &quot;Modelling customer relationships as Markov Chains&quot; from P. E. Pfeifer and R. L. Carraway (2000). . Lets consider a business with non-contractual relations with their customers, it can be for instance a shop selling products. The business spends money on direct marketing campaings that aim to make customers who have purchased previously purchase again. . As the famous quote says &quot;Half the money I spend on advertising is wasted; the trouble is I don&#39;t know which half.&quot; (John Wanamake). The purpose of the work analized here is to model the customer purchasing behavior in the scenario depicted above, with the objective of optimizing the direct marketing spend of the business. . The authors start by presenting a simple toy scenario to illustrate their ideas. I will present below this simple scenario, with some modifications to make things more concrete. But before doing this, I will like to introduce briefly two concepts that we will be using: Customer Lifetime Value and Markov Chains. After presenting the toy model, we will discuss a general formulation of the model. . Customer Lifetime Value . Customer Lifetime Value, which we will be referring to as Lifetime Value (LV), is a very important concept for marketing. LV is a prediction of the profit attributed to the future relationship with a customer. . This short definition tells us that LV is the expected future profit from a customer relationship and as such it needs to be a prediction, in other words it will depend on some model and assumptions. Being a forward looking metric, its useful as it allows managers to focus on customer that will be more profitable in the long term. LV can also be regarded as an upper limit on the cost-per-acquisition of new customers. . Other definitions mention that LV represents the present value of the projected future cash flows attributed to a customer relationship, and by this they are taking into account the time-value of money. . When modelling the LV we need to consider wether we are dealing with a contractual setting (for example a subscription based business model) or a non-contractual setting (like a shop selling items online). The models used will differ considerably in both cases. . Markov Chains . When we observe a sequence of chance experiments, the probabilities for the outcomes of the next experiment can depend in general on all of the past outcomes. Here we focus on a special class of processes that present a simpler dependence between experiments. Processes for which the outcome of a given experiment can only affect the outcome of the next experiment are known as Markov Chains. This naming comes from A. A. Markov, who studied the mathematical properties of these processes. An introduction to Markov Chains can be found on this book for example Grinstead and Snell’s Introduction to Probability. . We specify a Markov Chain in terms of a set of states $S = {s_1, s_2, ldots s_n }$. The system will move among these states in discrete steps. When the chain is in the state $s_i$, it will move to the state $s_j$ at the next step with a probability $p_{ij}$. We call $p_{ij}$ transition probabilities. . Markov chains are also depicted usually in terms of a transition diagram, below we show a simple Markov chain . . The frog can be in two states (left lily or right lily) and the numbers depict the associated transition probabilities. We can alternatively see the Markov Chain as a matrix . $$ begin{array}{c c} &amp; begin{array}{c c} L &amp; R end{array} begin{array}{c c} L R end{array} &amp; left[ begin{array}{c c} 0.3 &amp; 0.7 0.5 &amp; 0.5 end{array} right] end{array} $$where we denoted by L (R) the left (right) flowers. . Toy model . Consider a shop that sells one product at a price $NC$ (in the notation of the authors, $NC$ is the net contribution to the company profits). Assume for simplicity that customers cannot buy more than one unit per month from this shop. Every month the company will spend $M$ in marketing for each customer unless its last purchase was 5 months ago or longer, when this happens the shop considers the relation with the customer finished. . We will be characterizing customers in terms of states. In this case, customer states will be determined by their recency, the number of months since their last purchase. Lets consider a concrete case: We have a new customer that purchases in january. In february we will say that this customer has a recency $r=1$, as one month as passed. On march, he will continue having recency $r=1$ if he bought again in february, otherwise he will have recency $r=2$. If the customer has not bought anything by june, he will have recency $r=5$ by then and we will declare him inactive at that point. . In the toy scenario we analyze, we will assume that the probability that a customer will purchase again in a given month only depends on its recency. Direct marketers have observed that recency plays a very important role in determining the probability that a customer will purchase again, so though we analyze a very simplified scenario it is not pointless. . The system we are describing is very special, in the sense that the next state of the customer is determined only by its current state indepently of how he arrived to that state. For instance, a customer with recency $r=3$ can either purchase again or not. The probability he purchases again in that month only depends on its recency. If he purchases again his next state is $r=1$, else his next state is $r=4$. This is what people calls the Markov property. . If we draw this system as a diagram it would look like this . . The arrows represent the possible paths and $p_r$ represents the probability that a customer with recency $r$ will purchase again. In matrix form, the previous diagram can be written as . begin{equation*} mathbf{P} = begin{pmatrix} p_1 &amp; 1 - p_{1} &amp; 0 &amp; 0 &amp; 0 p_2 &amp; 0 &amp; 1- p_2 &amp; 0 &amp; 0 p_3 &amp; 0 &amp; 0 &amp; 1-p_3 &amp; 0 p_4 &amp; 0 &amp; 0 &amp; 0 &amp; 1-p_4 0 &amp; 0 &amp; 0 &amp; 0&amp; 1 end{pmatrix} end{equation*}Note that the $[ mathbf{P}]_{5,5}$ entry is $1$ as the customer will never purchase back again in this state. Ultimately, we would like to estimate the lifetime value of our customers. For this, we need to consider the cash-flows associated to each customer state. This information is contained in the vector $ mathbf{R}$ . begin{equation*} mathbf{R} = begin{pmatrix} NC - M -M -M -M 0 end{pmatrix} end{equation*}Which can be interpreted as follows, when the customer goes to state $r=1$, he has just purchased one item (bringing $+ NC$ in profits) and will cost $M$ in marketing spenditures in that month. For recency $r=2,3,4$ the customer costs $M$ in marketing while for $r=5$ the shop already declared the customer inactive. . With all the information we have so far, we can estimate the Lifetime Value (LV) of a customer $T$ months after its first purchase. Assume that we have a customer making its first purchase in a given month. The LV at $T$ months is then given by . begin{equation*} mathrm{LV} = [ sum_{t=0}^{T} mathbf{P}^{ ,t}]_{1i} , R_{i} end{equation*}Here we use Einstein summation convention of contracted indices. We can examine the first two terms. The first term is simply $NC - M$ corresponding to the first purchase and the associated first marketing spenditure on that customer. The second term is $(p_1) (NC - M ) - (1-p_1) M $, which accounts for the two possible scenarios at this stage (he buys again or not). This hopefully makes the previous formula more clear. Note that $[ mathbf{P}^{ ,t}]_{ij}$ is the probability that the customer will be at recency $j$ at the end of month $t$ given that he started at recency $i$. Thats one of the nice things of the Markov property, we can know what happens after several steps by simply taking powers of the one-step evolution matrix. . Notice that in all the discussion I am not including the effects of the time-value of money as the authors do, as I dont think they are crucial for the dicussion and then I can deal with simpler formulas. . We can take the previous formula for the LV and consider the limit $T to infty$. In this case the formula becomes . begin{equation*} mathrm{LV} = [(1- mathbf{P})^{-1}]_{1i} , R_{i} end{equation*}where we have used Taylor expansion for the inverse matrix of $1- mathbf{P}$. . We have a Markov Chain with five states $ { s_1, s_2, ldots, s_5 }$ for the different recency values. The last state $r=5$ is known as an absortive state because it is impossible to leave it. Furthermore we are dealing with an absortive Markov Chain because its possible to go from all states to the absortive state. Absortive Markov Chains have some special properties we will be using next. . Note that we can write our matrix $ mathbf{P}$ in block form as . begin{equation*} mathbf{P} = begin{pmatrix} Q_{4 times 4}&amp; A_{4 times 1} 0_{1 times 4} &amp; 1 end{pmatrix} end{equation*}$Q$ encodes transition probabilities among transient states and $A$ represents absortive probabilities. Taking also into account also that $R_{5} = 0$, we can write . begin{equation*} mathrm{LV} = sum_{i=1}^{4} [(1- mathbf{Q})^{-1}]_{1i} , R_{i} end{equation*}At this stage we can resort to the properties of absortive Markov Chains, which tell us that $1- mathbf{Q}$ has an inverse given by . begin{equation*} N equiv (1 - Q)^{-1} = 1 + Q + Q^2 + cdots end{equation*}$N$ is called the fundamental matrix of $ mathbf{P}$. We also know that $ lim_{n to infty} Q^n = 0$, meaning that the process will always be absorbed at some point. . Having a model of the customer LV, we can optimize the marketing campaign according to this value. Lets consider some specific values for the different model parameters: $NC= 40$, $M=4$. Lets also consider a situation where we observe $p_1=0.3$, $p_2=0.2$, $p_3=0.15$, $p_4=0.05$. In this case we obtain $ mathrm{LV} = 64.25$. Now, lets imagine the firm tries a different approach where they close the relation with the customers at $r geq 4$ (this implies $R_4 = 0$), and lets say this has the effect of decreasing $p_4$ down to $0.01$. In this case we get $ mathrm{LV} = 65.7$. We can then choose among different marketing campaign settings by optimizing on the LV. . General Formulation . In this section we will present a general model for customer relationships as Markov Chains. The paper by P. E. Pfeifer and R. L. Carraway presents different sample applications of their ideas with increasing level of complexity, but doesnt present a common generic framework for their model. Its very useful to have such general formulation in order to have a common language to formulate generic results that apply to all particular cases. I will try to present such a formulation here. . First, we will model the customer relationship with an absortive Markov Chain. There will be $r$ transient states and $t$ absortive states. The customers move between these $(r+t)$ states in time steps with some determined duration (previously in the toy example we used months as the time period). When the customers reach the absortive states, they dont transition any further. We will have a transition matrix with the block form . begin{equation*} mathbf{P} = begin{pmatrix} Q_{t times t}&amp; A_{t times r } 0_{r times t } &amp; 1_{r times r} end{pmatrix} end{equation*}$Q$ stands for the probability of transition among the $r$ transient states and $A$ is the probability of transition to the $t$ absortive states. We assume $r, t &gt; 0$. Note that powers of this matrix keep the same block structure . begin{equation*} mathbf{P}^n = begin{pmatrix} Q^n&amp; * 0 &amp; 1 end{pmatrix} end{equation*}where $*$ is some non-null entry we dont write explicitly. We will also have a vector summarizign the cash flows of each state. This vector will be denoted by $R$ and will have $t+r$ dimensions. . The absortive states will correspond to states where the customer is considered inactive and no marketing spend is present, so the last $r$ entries of $R$ will be null. The lifetime value is given by . begin{equation*} mathrm{LV} = sum_{i=1}^{t} N_{1i} , R_{i} end{equation*}where $N equiv (1 - Q)^{-1} = 1 + Q + Q^2 + cdots$ is the fundamental matrix of $ mathbf{P}$. The toy model presented previously as well as the other examples analyzed by P. E. Pfeifer and R. L. Carraway in their article can be cast as particular cases of this framework. Note that since $ lim_{n to infty} Q^n = 0$, we can compute the fundamental matrix by including a finite number of terms in the series until we reach a satisfactory accuracy. .",
            "url": "https://celis.github.io/personal/jupyter/2020/08/06/customer-relationships-as-markov-chains.html",
            "relUrl": "/jupyter/2020/08/06/customer-relationships-as-markov-chains.html",
            "date": " • Aug 6, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Flask MNIST",
            "content": "Introduction . On this project I built a simple web application motivated by the repository pytorch-flask-api-heroku. The project pytorch-flask-api-heroku builds a web application for classifying images that can be uploaded via a web form. The web application is deployed in Heroku and uses a pre-trained model from PyTorch for the image classification. . My objective was to build a similar web application hosting an image classifier trained on the MNIST dataset. The MNIST dataset consist of a collection of handwritten digit images. The digits have been size-normalized and centered in a 28x28 pixel box. . . The following repository trains a Logistic Regression classifier on the MNIST dataset and uploads the model artifact to AWS S3. On this part I used as reference the tutorial What is torch.nn really?. . Last, this repository contains the Flask web application. It downloads the model artifact from AWS S3 and predicts the digit of an image uploaded via a web form. The web application was deployed in Heroku. . Demonstration . Below I provide a demonstration of the application. . .",
            "url": "https://celis.github.io/personal/jupyter/2020/03/22/flask-mnist.html",
            "relUrl": "/jupyter/2020/03/22/flask-mnist.html",
            "date": " • Mar 22, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Hep Recommender",
            "content": "Introduction . In this note I want to discuss hep-recommender, a recommender system for scientific papers in the field of High Energy Physics (HEP) aimed to help researchers in the exploration of relevant literature. This is a personal project I have been working recently in collaboration with José Eliel Camargo Molina. A submission of this project to the Facebook Artificial Intelligence Hackathon (2020) won the third place. . There are different approaches to determine the degree of similarity between articles in order to identify related work. Some works have used text-mining and natural language processing methods. Another popular approach is based on citation analysis, where the similarity between two articles is estimated based on bibliographic information [1,2,4,5]. Finally, other works have used a combination of the previous approaches, see for instance [3]. . In this project, we develop a recommender system based on citation analysis. More specifically, we will develop a recommender system that elaborates on the ideas of co-citation analysis [1,4] and co-citation proximity analysis (CPA) [2]. Co-citation analysis is based on the premise that articles which are frequently cited together (by the same papers) should be related to each other. CPA extends this idea by incorporating the notion that the closer the citations are to each other within the article text, the more likely it is that they are related. While these methods are relatively simple they provide a high quality of related article recommendations. Our recommender system relies on a distributed representation of articles obtained by training a Skip-Gram model on reference lists. This model also captures the notion that articles cited close to each other on the text are similar. . In this note I want to go to some detail about the model powering our recommender system, how it compares to other approaches, and how we made it available via a web application. . Related work . I would like to start discussing some of the standard methods in the literature to quantify the similarity between two articles. I will define the following concepts: . bibliometric coupling . | co-citation count . | co-citation proximity analysis (CPA) . | . Bibliometric couplign reflects the idea that two papers sharing a large portion of their references should be similar [5]. Co-citation count assigns a high degree of similarity to articles which are frequenly cited together (regardless of where the citations occcured within the text) [1]. CPA extends co-citation count by assigning more weigth to cases where the articles are cited close to each other within the text [2]. In general CPA is expected to give better results than co-citation count, though it requires some work in order to parse the references of each article keeping information about the location of the references within the text. . Lets formalize these concepts. We consider a set of papers $ { omega_1, omega_2, ldots omega_n }$. We define the quantities $c_{i,j}$, where $c_{i,j}=1$ if $ omega_i$ is cited by $ omega_j$ and $c_{i,j}=0$ otherwise. Then we are ready to introduce our three measures of similarity: . bibliographic coupling is defined as $ sum_{k=1}^{n} c_{k,i} c_{k,j}$ | . co-citation count is defined as $ sum_{k=1}^{n} c_{i,k} c_{j,k}$ | . CPA can be used to define a measure $ sum_{k=1}^{n} eta_{i,j}^{k} c_{i,k} c_{j,k}$, where $n_{ij}^{k}$ are coefficients that penalize cases where the citations occur far from each other within the text. | . In the original CPA article [2], $ eta$ was fixed to $1$ if the citations occur in the same sentence, $1/2$ if they only occur in the same paragraph, $1/4$ if they only occur in the same chapter, and $1/8$ if they only appear on the same article. Parametric representations of $ eta$ have also been proposed [7]. . Distributed representation on co-citations . We propose a model that relies on the same idea behind CPA, namely, that the proximity of references within an article provide valuable information regarding the similarity of two articles. However, we wanted to simplify the process of extracting the necessary data as much as possible. . We assume that it is possible to extract the list of references of each article in the order of appearance within the text. For instance the following article . Higgs boson pair production in gluon fusion is the most promising process to find out whether the Higgs boson self-coupling is Standard-Model-like. Early studies of Higgs boson pair production within an EFT framework can be found in Refs. [1–3]. Many phenomenological investigations about the potential of this process to reveal New Physics have been performed since, see e.g. Refs. [4–8]... . would give rise to a reference list [1,2,3,4,5,6,7,8,...]. In this way, articles which are frequently close to each other on these reference lists should be very similar, as the community is citing them close to each other within the text of the articles. . We will use an approach that has proven to be very fruitful in Natural Language Processing (NLP). We will take the lists of references and train a Skip-Gram model, such that articles which tend to be cited close to each other will have similar embeddings. . Our dataset consists of ordered lists of references. Suppose we have a total of $n$ unique articles in our dataset and for each of these articles we have its reference list. The Skip-gram model maximizes the following objective function: begin{align} frac{1}{n} sum_{q=1}^{n} left[ frac{1}{n_q} sum_{i=1}^{n_q} sum_{ -c leq j leq c , , j neq 0 } log p(w_{i+j}^{q}| w_i^{q}) right] end{align} . Here $ omega_{i}^{q}$ represents the embedding of the $i$-th article appearing on the reference list of article $q$; $n_q$ represents the size of the reference list for article $q$, and $c$ represents the context-window size of the Skip-Gram model. The probability is modelled using a softmax function . begin{align} p( omega_{i} | omega_j ) = frac{ exp( omega_{i} cdot omega_{j} )}{ sum_{l=1}^{n} exp( omega_{l} cdot omega_j ) } end{align}Just like in the Skip-Gram model used in NLP, in order to make the model training more efficient we need to implement negative sampling or hierarchical softmax. A good implementation of the Skip-Gram model is provided by the Gensim library for instance. . In order to explore the structure of the trained embeddings, I took a sample of articles and made a t-SNE visualization of their embeddings in two dimensions: . . For this, I picked a set of articles that were published on the arXiv. Each arXiv category was displayed with a different color. On this plot, some of the arXiv categories that appear are: astro-ph (red), hep-ex (blue), hep-th (black) and hep-ph (green), among others. As expected, the embeddings form cluster around their categories. . Data . Regarding open acces digital libraries, the research community in the field of High Energy Physics mainly uses the INSPIRE-HEP and the arXiv. INSPIRE-HEP provides an API from which data for articles in the field of HEP can be retrieved, we use this API to extract the data we need. Below I show some exploratory example. Lets first define some classes that talk to the API and handle the response. . import requests from typing import List, Dict class InspireAPI: &quot;&quot;&quot; Simple wrapper class around the INSPIRE API https://inspirehep.net methods: literature: gives access to the literature endpoint &quot;&quot;&quot; LITERATURE = &quot;https://inspirehep.net/api/literature/&quot; def __init__(self): pass def literature(self, record_id: str): &quot;&quot;&quot; Returns api response for a given record_id &quot;&quot;&quot; url = self.LITERATURE + record_id return LiteratureRecord(requests.get(url).json()) class LiteratureRecord: &quot;&quot;&quot; Datamodel class for handling literature record data, implementing basic methods to access the properties &quot;&quot;&quot; def __init__(self, data: Dict): self.data = data @property def record_id(self) -&gt; str: &quot;&quot;&quot; Returns the INSPIRE id of the article &quot;&quot;&quot; return self.data[&#39;id&#39;] @property def metadata(self) -&gt; Dict: &quot;&quot;&quot; Returns article metadata &quot;&quot;&quot; return self.data[&#39;metadata&#39;] @property def references(self) -&gt; List[str]: &quot;&quot;&quot; Returns reference list of the article as a List of INSPIRE article ids &quot;&quot;&quot; if self.metadata.get(&quot;references&quot;): return [ element[&quot;record&quot;][&quot;$ref&quot;].split(&quot;/&quot;)[-1] for element in self.metadata[&quot;references&quot;] if element.get(&quot;record&quot;) ] . We can now look at one example. All the bibliographic data is contained on the metadata property of our LiteratureRecord class. I implemented a references property in order to retrieve the list of references (only the article identifiers). Lets extract data for the article with the identifier &#39;11883&#39;, . inspireapi = InspireAPI() record = inspireapi.literature(&#39;11883&#39;) record.references . [&#39;40440&#39;, &#39;12289&#39;, &#39;12290&#39;, &#39;14006&#39;, &#39;12291&#39;, &#39;12288&#39;, &#39;9159&#39;, &#39;43800&#39;, &#39;43801&#39;] . In this way we are able to extract the lists of references we need for our model. Note that the INSPIRE-API returns references in the order of appearance on the reference section of the article, and by tradition, the HEP community orders references by the order of appearance on the text. . Recommendations . We can use the Skip-Gram model embeddings and some metric on the vector space to provide recommendations for articles with at least some citations. Very recent articles or unpopular articles with no citations will not have embeddings and we must find other method to provide recommendations for these articles. For this reason, we consider two scenarios when we want to provide recommendations for a given article: . i) The article has embeddings produced by our Skip-Gram model. . ii) The article has no embeddings from the Skip-Gram model. . In case i) we retrieve the top similar articles using cosine similarity as the metric. Cosine similarity is defined as $ cos theta_{12} = hat omega_{1} cdot hat omega_{2}$, where $ hat omega = omega/| omega|$ is a unit norm vector. In this case we retrieve as recommendations the articles whose embedding have a smaller angle difference with the original article. . In case ii), we dont have an embedding but we can build one out of the references of the article. We retrieve the references of the article by calling the INSPIRE-HEP api and then take the average vector $ omega_{ rm{avg}}^q = 1/n_q sum_{i=1}^{n_q} omega_i^{q}$ of the references. We can then proceed to retrieve the top similar articles using cosine similarity as before. . These two simple methods are able to provide recommendations for most articles. . Web application . The recommender system has been made available at hep-recommender. It is a Flask web application currently hosted on Heroku with a gunicorn server. Storage of the model artifacts is done in AWS S3. . Discussion . To see one example of the application, we can look the recommendations of similar articles for: . &quot;Broken Symmetries and the Masses of Gauge Bosons&quot; by Higgs, Peter W. (1964). . This article is one of the works behind the 2013 Physics nobel prize for the theoretical development of the so-called Higgs mechanism. The recommendations from our system are very good, including the other articles which were also responsible for these theoretical developments as aknowledged by the community. . Another interesting example is this one . &quot;Unitary Symmetry and Leptonic Decays&quot; by Cabibbo, Nicola (1963) . This article introduced the idea of quark mixing when only two quark generations were known. The recommendations include the article . &quot;CP Violation in the Renormalizable Theory of Weak Interaction&quot; by Kobayashi, Makoto; Maskawa, Toshihide (1973) . which extends the idea of Cabibbo to three quark generations and gave rise to what is now known as the CKM matrix. . Exploring a large set of articles I was surprised by the high quality of the recommendations. Several experts on the field of HEP have also been using the application and find the recommendations satisfactory. . References . [1] H. Small, “Co-citation in the scientific literature: A new measure of the relationship between two documents” Journal of the American Society for Information Science, vol. 24, no. 4, pp. 265–269, 1973. . [2] B. Gipp and J. Beel, “Citation proximity analysis (cpa) : A new approach for identifying related work based on co-citation analysis” in Proceedings of the 12th International Confer- ence on Scientometrics and Informetrics, vol. 1 (B. Larsen, ed.), (Sao Paulo), pp. 571–575, BIREME/PANO/WHO, 2009. . [3] A. Kanakia, Z. Shen, D. Eide, and K. Wang, “A scalable hybrid research paper recommender system for microsoft academic” CoRR, vol. abs/1905.08880, 2019. . [4] I. V. Marshakova-shaikevich, “System of document connections based on references” 2009. . [5] M. M. Kessler, “Bibliographic coupling between scientific papers” American Documentation, vol. 14, no. 1, pp. 10–25, 1963. . [6] T. Mikolov, K. Chen, G. S. Corrado, and J. Dean, “Efficient estimation of word representations in vector space” CoRR, vol. abs/1301.3781, 2013. . [7] M. Schwarzer, M. Schubotz, N. Meuschke, C. Breitinger, V. Markl, and B. Gipp, “Evaluating link-based recommendations for wikipedia” 2016 IEEE/ACM Joint Conference on Digital Libraries (JCDL), pp. 191–200, 2016. .",
            "url": "https://celis.github.io/personal/jupyter/2020/02/20/hep-recommender.html",
            "relUrl": "/jupyter/2020/02/20/hep-recommender.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "I use this site to write technical notes about personal projects and different topics. Details about me can be found here. .",
          "url": "https://celis.github.io/personal/about",
          "relUrl": "/about",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://celis.github.io/personal/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}